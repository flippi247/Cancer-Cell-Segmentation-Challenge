{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da43841",
   "metadata": {
    "executionInfo": {
     "elapsed": 1371,
     "status": "ok",
     "timestamp": 1654250535193,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "2da43841"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from skimage.measure import label, regionprops\n",
    "import pickle\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision.ops import nms\n",
    "from torchmetrics.detection import MeanAveragePrecision\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import platform\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from skimage.filters import gabor, gabor_kernel\n",
    "from skimage.morphology import dilation, erosion\n",
    "from scipy.signal import convolve2d\n",
    "import imutils\n",
    "from imutils import contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7339d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfcb2a71",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1654250535986,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "dfcb2a71"
   },
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, annotations_dict, slide_names, path_to_slides, crop_size = (128,128), pseudo_epoch_length:int = 1000, transformations = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.anno_dict = annotations_dict\n",
    "        self.slide_names = slide_names\n",
    "        self.path_to_slides = path_to_slides\n",
    "        self.crop_size = crop_size\n",
    "        self.pseudo_epoch_length = pseudo_epoch_length\n",
    "        \n",
    "        # list which holds annotations of all slides in slide_names in the format\n",
    "        # slide_name, annotation, label, min_x, max_x, min_y, max_y\n",
    "        \n",
    "        self.slide_dict, self.annotations_list, self.slide_dict_segmented = self._initialize()\n",
    "        self.sample_cord_list = self._sample_cord_list()\n",
    "\n",
    "        # set up transformations\n",
    "        if transformations is not None:\n",
    "            self.transformations = transformations\n",
    "        else:\n",
    "            self.transformations = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "    def _initialize(self):\n",
    "        # open all images and store them in self.slide_dict with their name as key value\n",
    "        slide_dict = {}\n",
    "        slide_dict_segmented = {}\n",
    "        annotations_list = []\n",
    "        for slide in os.listdir(self.path_to_slides):\n",
    "            if slide in self.slide_names:\n",
    "                im_obj = Image.open(os.path.join(self.path_to_slides, slide)).convert('RGB')\n",
    "                slide_dict[slide] = im_obj\n",
    "                # setting up a list with all bounding boxes\n",
    "\n",
    "\n",
    "                # Building Segmented Image\n",
    "                segmented = np.zeros((im_obj.size[1], im_obj.size[0]), dtype=np.uint8)\n",
    "                for annotation in self.anno_dict[slide]:\n",
    "                    x_y_list = [(self.anno_dict[slide][annotation]['x'][i], self.anno_dict[slide][annotation]['y'][i]) for i in range(len(self.anno_dict[slide][annotation]['y']))]\n",
    "                    cv2.fillPoly(segmented, pts=np.array([x_y_list]), color=(self.anno_dict[slide][annotation]['class']))\n",
    "\n",
    "                slide_dict_segmented[slide] = segmented\n",
    "\n",
    "                for annotation in self.anno_dict[slide]:\n",
    "                    max_x, min_x = max(self.anno_dict[slide][annotation]['x']), min(self.anno_dict[slide][annotation]['x'])\n",
    "                    max_y, min_y = max(self.anno_dict[slide][annotation]['y']), min(self.anno_dict[slide][annotation]['y'])\n",
    "                    # since 0 is always the background class\n",
    "                    label = self.anno_dict[slide][annotation]['class']\n",
    "\n",
    "                    annotations_list.append([slide, annotation, label, min_x, min_y, max_x, max_y])\n",
    "\n",
    "        return slide_dict, annotations_list, slide_dict_segmented\n",
    "\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        slide, x_cord, y_cord = self.sample_cord_list[index]\n",
    "        x_cord = np.int64(x_cord)\n",
    "        y_cord = np.int64(y_cord)\n",
    "        # load image\n",
    "        img = self.slide_dict[slide].crop((x_cord,y_cord,x_cord + self.crop_size[0],y_cord + self.crop_size[1]))\n",
    "\n",
    "        segmented = (self.slide_dict_segmented[slide])[y_cord:y_cord+self.crop_size[1], x_cord:x_cord+self.crop_size[0]]\n",
    "        # transform image\n",
    "        img = self.transformations(img)\n",
    "        \n",
    "        # load boxes for the image\n",
    "        labels_boxes = self._get_boxes_and_label(slide,x_cord,y_cord)\n",
    "        # check if there is no labeld instance on the image\n",
    "        if len(labels_boxes) == 0:\n",
    "            labels = torch.tensor([0], dtype = torch.int64)\n",
    "            boxes = torch.zeros((0,4),dtype = torch.float32)\n",
    "        else:\n",
    "            labels = torch.tensor([line[0]-1 for line in labels_boxes], dtype=torch.int64).to(device)\n",
    "            # now, you need to change the originale box cordinates to the cordinates of the image\n",
    "            boxes = torch.tensor([[line[1] - x_cord, line[2] - y_cord, line[3] - x_cord, line[4] - y_cord] for line in labels_boxes],dtype=torch.float32).to(device)\n",
    "        \n",
    "\n",
    "        target = {\n",
    "            \"boxes\" :boxes,\n",
    "            \"labels\": labels,\n",
    "            \"segmentation\": torch.from_numpy(segmented).type(torch.int64).to(device)\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "        \n",
    "\n",
    "    def _sample_cord_list(self):\n",
    "        # select slides from which to sample an image\n",
    "        slides = random.choice(self.slide_names, size = self.pseudo_epoch_length, replace = True)\n",
    "        # select coordinates from which to load images\n",
    "        # only works if all images have the same size\n",
    "        width,height = self.slide_dict[slides[0]].size\n",
    "        cordinates = random.randint(low = (0,0), high=(width - self.crop_size[0], height - self.crop_size[1]), size = (self.pseudo_epoch_length,2))\n",
    "        return np.concatenate((slides.reshape(-1,1),cordinates), axis = -1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pseudo_epoch_length\n",
    "\n",
    "    def _get_boxes_and_label(self,slide,x_cord,y_cord):\n",
    "        return [line[2::] for line in self.annotations_list if line[0] == slide and line[3] > x_cord and line [4] > y_cord and line[5] < x_cord + self.crop_size[0] and line[6] < y_cord + self.crop_size[1]]\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        \"\"\"\n",
    "        Since each image may have a different number of objects, we need a collate function (to be passed to the DataLoader).\n",
    "        This describes how to combine these tensors of different sizes. We use lists.\n",
    "        Note: this need not be defined in this Class, can be standalone.\n",
    "        :param batch: an iterable of N sets from __iter__()\n",
    "        :return: a tensor of images, lists of varying-size tensors of bounding boxes, labels, and difficulties\n",
    "        \"\"\"\n",
    "\n",
    "        images = list()\n",
    "        segmentations = list()\n",
    "        targets = list()\n",
    "\n",
    "        for b in batch:\n",
    "            images.append(b[0])\n",
    "            segmentations.append(b[1][\"segmentation\"])\n",
    "            targets.append(b[1])\n",
    "            \n",
    "        images = torch.stack(images, dim=0).to(device)\n",
    "        segmentations = torch.stack(segmentations, dim=0).to(device)\n",
    "        \n",
    "        return images, segmentations, targets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5501899c",
   "metadata": {
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1654250550198,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "5501899c"
   },
   "outputs": [],
   "source": [
    "path_to_slides = 'AgNOR_ROI/'\n",
    "annotations = pickle.load(open(path_to_slides+\"annotations_dict_train.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d93af097",
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1654250552617,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "d93af097"
   },
   "outputs": [],
   "source": [
    "# slides are the filenames of the train images\n",
    "slides = list(annotations.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d52e3f07",
   "metadata": {
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1654250564607,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "d52e3f07"
   },
   "outputs": [],
   "source": [
    "batch_size=4\n",
    "\n",
    "# setting up datasets\n",
    "training_dataset = Dataset(annotations,slide_names=[slides[0], slides[2]],path_to_slides = path_to_slides ,crop_size=(256,256), pseudo_epoch_length=1000)\n",
    "validation_dataset = Dataset(annotations,slide_names=[slides[1]],path_to_slides = path_to_slides ,crop_size=(256,256), pseudo_epoch_length=1000)\n",
    "\n",
    "# setting up dataloaders\n",
    "train_loader = DataLoader(training_dataset,batch_size=batch_size,collate_fn=training_dataset.collate_fn)\n",
    "val_loader = DataLoader(validation_dataset,batch_size=batch_size,collate_fn=validation_dataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf6ddfb8",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1654250564921,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "cf6ddfb8"
   },
   "outputs": [],
   "source": [
    "class UNetEncoderBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, pad=1, stride=2, first_layer=False, use_incr=False):\n",
    "        super().__init__()\n",
    "        incr = 0 if (first_layer != True or use_incr == False) else 4\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Identity() if first_layer else nn.ReLU(), \n",
    "            nn.Conv2d(c_in, c_out, (k + 1 + incr), padding=(pad + incr // 2), stride=stride), \n",
    "            nn.Identity() if first_layer else nn.InstanceNorm2d(c_out), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(c_out, c_out, k, padding=pad), \n",
    "            nn.InstanceNorm2d(c_out)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x\n",
    "    \n",
    "class UNetDecoderBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, pad=1, stride=2, last_layer=False, use_incr=False):\n",
    "        super().__init__()\n",
    "        incr = 0 if (last_layer != True or use_incr == False) else 4\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(c_in, c_in, (k + 1), padding=pad, stride=stride), \n",
    "            nn.InstanceNorm2d(c_in), \n",
    "            nn.ReLU(), \n",
    "            nn.Conv2d(c_in, c_out, (k + incr), padding=(pad + incr // 2)), \n",
    "            nn.Identity() if last_layer else nn.InstanceNorm2d(c_out), \n",
    "            nn.Identity() if last_layer else nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, hidden_size=1024, c_in=3, c_out=7):\n",
    "        super().__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            UNetEncoderBlock(c_in, (hidden_size//8), first_layer=True),\n",
    "            UNetEncoderBlock(hidden_size//8, hidden_size//4),\n",
    "            UNetEncoderBlock(hidden_size//4, hidden_size//2),\n",
    "            UNetEncoderBlock(hidden_size//2, hidden_size),\n",
    "        ])\n",
    "        self.decoders = nn.ModuleList([\n",
    "            UNetDecoderBlock(hidden_size, hidden_size//2),\n",
    "            UNetDecoderBlock(2*hidden_size//2, hidden_size//4),\n",
    "            UNetDecoderBlock(2*hidden_size//4, hidden_size//8),\n",
    "            UNetDecoderBlock((2*hidden_size//8), c_out, last_layer=True),\n",
    "        ])\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        encodings = []\n",
    "        for i, encoder in enumerate(self.encoders):\n",
    "            x = encoder(x)\n",
    "            encodings.insert(0, x)\n",
    "\n",
    "        for i, decoder in enumerate(self.decoders):\n",
    "            if i > 0:\n",
    "                x = torch.cat((x, encodings[i]), axis=1)\n",
    "            x = decoder(x)\n",
    "        \n",
    "        return x.softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6d185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_border_bboxes(bbox_list):\n",
    "    bbox_new = []\n",
    "    for bbox in bbox_list:\n",
    "        tmp = [int(x) for x in bbox]\n",
    "        # bbox = [x_min, y_min, x_max, y_max]\n",
    "        if (0 in tmp or  256 in tmp):\n",
    "            continue\n",
    "        else:\n",
    "            bbox_new.append(bbox)\n",
    "    return bbox_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "686982e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_contours(img):\n",
    "    cnts = cv2.findContours(img.type(torch.uint8).numpy(), cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "    cnts = imutils.grab_contours(cnts)\n",
    "    if len(cnts) == 0:\n",
    "        return []\n",
    "    cnts = contours.sort_contours(cnts)[0]\n",
    "    # # loop over the contours\n",
    "    bboxes = []\n",
    "    for (i, c) in enumerate(cnts):\n",
    "        (x, y, w, h) = cv2.boundingRect(c)\n",
    "        if w * h >= 500:\n",
    "            bboxes.append([x, y, x+w, y+h])\n",
    "    bboxes = remove_border_bboxes(bboxes) \n",
    "    bboxes = torch.Tensor(bboxes)\n",
    "    # z = draw_bounding_boxes((images[0]*255).type(torch.uint8), bboxes)\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9c6bc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_forward(model, images):\n",
    "    seg_pred = model(images)\n",
    "    seg_pred_to_channels = F.one_hot(seg_pred.argmax(dim=1)).permute(0, 3, 1, 2).detach().cpu()[:, 1:, :, :]  # bs x 6 x 256 x 256 \n",
    "    predictions = []\n",
    "    for pred in seg_pred_to_channels:  # pred: 6 x 256 x 256 \n",
    "        prediction = {}\n",
    "        for i, pred_class in enumerate(pred):  # pred_class: 256 x 256\n",
    "            bboxes = cv_contours(pred_class)  # bboxes: [[1, 1, 1, 1], [2, 3, 3 ,2]]\n",
    "            if len(bboxes) > 0:\n",
    "                prediction[\"boxes\"] = bboxes if \"boxes\" not in prediction.keys() else torch.concat([prediction[\"boxes\"], bboxes])\n",
    "                prediction[\"scores\"] = torch.ones((bboxes.shape[0])) if \"scores\" not in prediction.keys() else torch.concat([prediction[\"scores\"], torch.ones((bboxes.shape[0]))])\n",
    "                prediction[\"labels\"] = torch.ones((bboxes.shape[0])) * i if \"labels\" not in prediction.keys() else torch.concat([prediction[\"labels\"], torch.ones((bboxes.shape[0])) * i])\n",
    "        predictions.append(prediction)\n",
    "    return predictions, seg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d07071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_one_epoch(val_loader, model, device:str = 'cpu', epoch:int = 0):\n",
    "    metric = MeanAveragePrecision()\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _, targets) in enumerate(val_loader):\n",
    "            images = images.to(device)\n",
    "\n",
    "            predictions, _ = eval_forward(model, images)\n",
    "            \n",
    "            for idx,t in enumerate(targets):\n",
    "                if len(t[\"boxes\"]) == 0:\n",
    "                    targets[idx]['boxes'] = torch.tensor([[0,0,0,0]], dtype = torch.float32).to(device)\n",
    "            targets = [{'boxes': t[\"boxes\"].cpu(), 'labels': t[\"labels\"].cpu()} for t in targets]\n",
    "            metric.update(predictions,targets)\n",
    "            \n",
    "    metrics_values = metric.compute()\n",
    "    print(f\"mAP 50: {metrics_values['map_50']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8247f01",
   "metadata": {
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1654250566204,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "c8247f01"
   },
   "outputs": [],
   "source": [
    "model = UNet().to(device)\n",
    "epochs = 40\n",
    "optim = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "# lr_schedule = WarmupLinearLRSchedule(optim, init_lr=3e-5, peak_lr=3e-4, end_lr=5e-5, warmup_epochs=int(epochs*0.1), epochs=epochs)\n",
    "# pos_weight = torch.Tensor([1, 5, 5, 5, 5, 5]).to(device)\n",
    "pos_weight = torch.Tensor([1, 1, 1, 1, 1, 1]).to(device)\n",
    "from kornia.losses import FocalLoss\n",
    "loss_fn = FocalLoss(alpha=0.5, gamma=5., reduction=\"mean\")\n",
    "# loss_fn = nn.CrossEntropyLoss(weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76fb083",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3078,
     "status": "error",
     "timestamp": 1654250701002,
     "user": {
      "displayName": "Philip",
      "userId": "16943322938249538799"
     },
     "user_tz": -120
    },
    "id": "e76fb083",
    "outputId": "721a6ac7-ca63-43a2-addf-1be16e3821a5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "for e in range(epochs):\n",
    "    moving_loss = 0.\n",
    "    pbar = tqdm(train_loader)\n",
    "    for i, (images, segmentations, _) in enumerate(pbar):\n",
    "    # images, segmentations = d  # overfitting one example to see if model can learn\n",
    "        seg_pred = model(images)\n",
    "        loss = loss_fn(seg_pred, segmentations)\n",
    "        moving_loss += loss.item() / (i+1)\n",
    "        losses.append(moving_loss)\n",
    "#         if i % 10 == 0:\n",
    "#             plt.imshow(torch.cat([\n",
    "#                 torch.cat([s for s in seg_pred.argmax(dim=1).detach().cpu()], dim=1),\n",
    "#                 torch.cat([s for s in segmentations.cpu()], dim=1)\n",
    "#             ], dim=0))\n",
    "#             plt.show()\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        pbar.set_postfix(loss=moving_loss)\n",
    "#     print(\"Predicting on Val Set:\")\n",
    "#     lr_schedule.step()\n",
    "    \n",
    "    print(f\"======================================================================================\")\n",
    "    print(f\"========================================Epoch: {e}======================================\")\n",
    "    \n",
    "    torch.save(model.state_dict(), \"ckpt.pt\")\n",
    "    model.eval()\n",
    "    validation_one_epoch(val_loader, model, \"cuda\")\n",
    "    images, _, targets = next(iter(val_loader))\n",
    "    idx = 3\n",
    "    preds, seg_pred = eval_forward(model, images)\n",
    "    preds = preds[idx]\n",
    "    bboxes = targets[idx][\"boxes\"]\n",
    "    print(f'Target-Labels: {targets[idx][\"labels\"]}')\n",
    "    print(f'Predicted Labels: {preds[\"labels\"]}')\n",
    "    print(\"Num boxes real\", len(targets[idx][\"labels\"]))\n",
    "    print(\"Num boxes pred\", len(preds[\"labels\"]))\n",
    "    plt.imshow(draw_bounding_boxes((images[idx]*255).type(torch.uint8), bboxes).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(draw_bounding_boxes((images[idx]*255).type(torch.uint8), preds[\"boxes\"]).permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    plt.imshow(seg_pred.argmax(dim=1)[idx].detach().cpu())\n",
    "    plt.show()\n",
    "#     pbar = tqdm(val_loader)\n",
    "#     for i, (images, segmentations, _) in enumerate(pbar):\n",
    "#         seg_pred = model(images)\n",
    "#         loss = loss_fn(seg_pred, segmentations)\n",
    "#         if i % 50 == 0:\n",
    "#             plt.imshow(torch.cat([\n",
    "#                 torch.cat([s for s in seg_pred.argmax(dim=1).detach().cpu()], dim=1),\n",
    "#                 torch.cat([s for s in segmentations.cpu()], dim=1)\n",
    "#             ], dim=0))\n",
    "#             plt.show()\n",
    "#     print(\"Finished Prediction on Val Set\")\n",
    "    model.train()\n",
    "    print(\"======================================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511e604d",
   "metadata": {
    "id": "511e604d"
   },
   "outputs": [],
   "source": [
    "## Save model\n",
    "torch.save(model.state_dict(), \"model_3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191090f",
   "metadata": {},
   "source": [
    "### Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66a9f624",
   "metadata": {
    "id": "66a9f624",
    "outputId": "5da366dc-f6fc-4f74-de1b-936f2bc503dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load model\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(\"ckpt.pt\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d7200d",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6598f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_whole_images(model,test_slides:list, image_folder_path:Path, device:str = 'cuda', overlap:int=50, crop_size:tuple=(256,256), compute_map:bool=True, annotations:dict = None):\n",
    "    \"\"\"Evaluates a model on the whole image. All parts of the image are processed seperately, with an overlap of defined size. Detections are filtered by non maximal surpression.\n",
    "     If compute_map is False, only predictions are made, using the model passed.\n",
    "\n",
    "    Args:\n",
    "        model (_type_): Pytorch Model\n",
    "        test_slides (list): Name of the slides to perform evaluation on\n",
    "        image_folder_path (Path): path to the folder where the images are stored\n",
    "        device (str, optional): Which device to use for making predictions. Defaults to 'cuda'.\n",
    "        overlap (int, optional): Overlap of subsequent crops in Pixels. Defaults to 50.\n",
    "        crop_size (tuple, optional): Size of the crops which are passed to the model. Defaults to (256,256).\n",
    "        compute_map (bool, optional): Wether to compute maP. Defaults to True.\n",
    "        annotations (dict, optional): Pass the annotations dict if you want to calculate the map. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        If compute_map is False, only a dict holding the predictions for each image is returned. Otherwise, also a dict with metrics is returned\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([transforms.ToTensor()])\n",
    "    model.to(device)\n",
    "\n",
    "    if compute_map:\n",
    "        metric = MeanAveragePrecision()\n",
    "\n",
    "    results_boxes = {}\n",
    "    with torch.no_grad():\n",
    "        for slide_name in test_slides:\n",
    "            totalpredictions = {'boxes': [], 'labels': [], 'scores': []}\n",
    "            results_boxes[slide_name] = []\n",
    "            # process one image\n",
    "            img = Image.open(image_folder_path / Path(slide_name))\n",
    "            width, height = img.size\n",
    "            # sample crops with overlap\n",
    "            for xmin in tqdm(np.arange(0,width + 1, (crop_size[0] - overlap)), f'evaluating on image {slide_name}'):\n",
    "                for ymin in np.arange(0,height + 1, crop_size[1] - overlap):\n",
    "                    crop = img.crop((xmin, ymin, xmin+crop_size[0], ymin+crop_size[1])).convert('RGB')\n",
    "                    crop = transform(crop)\n",
    "\n",
    "                    \n",
    "                    # detect figures on crop\n",
    "                    predictions, _ = eval_forward(model, crop.unsqueeze(dim = 0).to(device))\n",
    "                    \n",
    "                    \n",
    "            \n",
    "                    # correct offset, so bring the coordinates back to the coordinate system of the whole image\n",
    "                    predictions[0]['boxes'] = torch.Tensor([[x1+xmin,y1+ymin,x2+xmin,y2+ymin] for x1,y1,x2,y2 in predictions[0]['boxes']])\n",
    "\n",
    "\n",
    "                    for det in predictions[0]['boxes']:\n",
    "                        totalpredictions['boxes'].append(det)\n",
    "                    for det in predictions[0]['scores']:\n",
    "                        totalpredictions['scores'].append(det)\n",
    "                    for det in predictions[0]['labels']:\n",
    "                        totalpredictions['labels'].append(det+1)\n",
    "\n",
    "            if (len(totalpredictions['boxes'])>0):\n",
    "                totalpredictions['boxes'] = torch.stack(totalpredictions['boxes']).to('cpu')\n",
    "                totalpredictions['labels'] = torch.stack(totalpredictions['labels']).to('cpu')\n",
    "                totalpredictions['scores'] = torch.stack(totalpredictions['scores']).to('cpu')\n",
    "            else: # stack does not work for empty arrays\n",
    "                totalpredictions['boxes'] = predictions[0]['boxes'].to('cpu')\n",
    "                totalpredictions['labels'] = predictions[0]['labels'].to('cpu')\n",
    "                totalpredictions['scores'] = predictions[0]['scores'].to('cpu') # empty anyways\n",
    "                \n",
    "           \n",
    "\n",
    "            for b,l,sc in zip(totalpredictions['boxes'], totalpredictions['labels'], totalpredictions['scores']):\n",
    "                results_boxes[slide_name].append([*b, l, sc])\n",
    "\n",
    "            # get the targets from the annotation data\n",
    "            if compute_map:\n",
    "                if annotations == None:\n",
    "                    print(f\"annotations dict required to compute the map!\")\n",
    "                    return None\n",
    "                else:\n",
    "                    # get targets from the annotations dict\n",
    "                    targets = get_targets(annotations,slide_name)\n",
    "                    # update matric with detections, made on the current image\n",
    "                    metric.update([totalpredictions],[targets])\n",
    "\n",
    "        # finally compute the Ap over all test images\n",
    "        if compute_map:\n",
    "            metric_values = metric.compute()\n",
    "            return metric_values, results_boxes\n",
    "        else:\n",
    "            return results_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6ab43ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_targets(annotations:dict,slide_name:str):\n",
    "    \"\"\"Returns a dict with boxes and labels in pytorch format.\n",
    "\n",
    "    Args:\n",
    "        annotations (dict): annotations dict\n",
    "        slide_name (str): name of the slide for which to return the boxes and labels\n",
    "\n",
    "    Returns:\n",
    "        (dict): Dict holding boxes and labels in pytorch format\n",
    "    \"\"\"\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for annotation in annotations[slide_name].values():\n",
    "        maxx,minx = max(annotation['x']), min(annotation['x'])\n",
    "        maxy,miny = max(annotation['y']), min(annotation['y'])\n",
    "        boxes.append([minx,miny,maxx,maxy])\n",
    "        labels.append(annotation['class'])\n",
    "\n",
    "    targets = {\n",
    "        'boxes': torch.tensor(boxes, dtype= torch.float32),\n",
    "        'labels': torch.tensor(labels, dtype = torch.int64)\n",
    "    }\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "202ce0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training Image: AgNOR_0484.tiff, AgNOR_0517.tiff\n",
    "## Validation Image: AgNOR_0622.tiff\n",
    "path_to_slides = Path('AgNOR_ROI/')\n",
    "annotations = pickle.load(open(path_to_slides / Path(\"annotations_dict_train.p\"),\"rb\"))\n",
    "test_slides = list(annotations.keys())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bca743e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating on image AgNOR_0622.tiff: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:02<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "metric_values, predictions = evaluate_on_whole_images(\n",
    "    model=model,\n",
    "    test_slides=[test_slides],\n",
    "    image_folder_path=path_to_slides,\n",
    "    annotations = annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd8a76f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_50 on Validation Image: 0.51250\n"
     ]
    }
   ],
   "source": [
    "print(f\"map_50 on Validation Image: {metric_values['map_50']:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15a97e8",
   "metadata": {},
   "source": [
    "## Final Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5deee194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AgNOR_0495.tiff', 'AgNOR_2876.tiff', 'AgNOR_2906.tiff', 'AgNOR_8581.tiff', 'AgNOR_9845.tiff']\n"
     ]
    }
   ],
   "source": [
    "path_to_slides = Path('AgNOR_ROI/test_images/')\n",
    "test_slides = os.listdir(path_to_slides)[:-1]\n",
    "print(test_slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8737251d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "evaluating on image AgNOR_0495.tiff: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  4.17it/s]\n",
      "evaluating on image AgNOR_2876.tiff: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  4.36it/s]\n",
      "evaluating on image AgNOR_2906.tiff: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  4.37it/s]\n",
      "evaluating on image AgNOR_8581.tiff: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  4.34it/s]\n",
      "evaluating on image AgNOR_9845.tiff: 100%|███████████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  4.37it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = evaluate_on_whole_images(\n",
    "    model=model,\n",
    "    test_slides=test_slides,\n",
    "    image_folder_path=path_to_slides,\n",
    "    annotations = annotations,\n",
    "    compute_map = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "c9abb338",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(predictions, open('test_predictions_index_1', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "EPIC.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
